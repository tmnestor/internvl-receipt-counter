# Base configuration
seed: 42
deterministic: true
debug: false
log_level: info

# Data configuration
data:
  image_size: 448  # Required for InternVL2
  batch_size: 2
  num_workers: 2
  augmentation: true
  class_distribution: [0.3, 0.3, 0.4]  # Distribution for 0, 1, 2+ receipts
  train_csv: datasets/synthetic_receipts/metadata.csv
  train_dir: datasets/synthetic_receipts/images
  val_csv: datasets/synthetic_receipts/metadata.csv
  val_dir: datasets/synthetic_receipts/images
  test_csv: datasets/synthetic_receipts/metadata.csv
  test_dir: datasets/synthetic_receipts/images

# Model configuration
model:
  name: "internvl2"
  # Absolute path to the pre-downloaded model
  pretrained_path: "/home/jovyan/nfs_share/tod/InternVL2_5-1B"  # Updated to cloud path
  use_8bit: false  # Disable 8-bit quantization to avoid issues
  classifier:
    hidden_dims: [1536, 768, 256]
    dropout_rates: [0.4, 0.3, 0.2]
    batch_norm: true
    activation: "gelu"
  num_classes: 3  # 0, 1, or 2+ receipts

# Training configuration
training:
  epochs: 20
  early_stopping:
    patience: 2
    min_delta: 0.001
  optimizer:
    name: "adamw"
    learning_rate: 5e-5
    backbone_lr_multiplier: 0.01  # Reduced from 0.1 to prevent catastrophic forgetting
    weight_decay: 0.01
    gradient_clip: 1.0  # Re-enabled for non-mixed-precision training
  scheduler:
    name: "cosine"
    warmup_epochs: 3
    min_lr_factor: 0.01
  loss:
    name: "cross_entropy"
    label_smoothing: 0.1
  mixed_precision: false  # Disabled due to FP16 gradient issues
  mixed_precision_dtype: "float16"  # Use "float16" for GradScaler compatibility
  # Acceleration options
  torch_compile: false  # Disable for now due to dtype compatibility issues
  compile_mode: "reduce-overhead"  # Options: "reduce-overhead", "max-autotune", or "default"
  compile_full_precision_only: true  # Only use compile when not using mixed precision
  flash_attention: true  # Use Flash Attention if available
  three_stage:
    enabled: true
    mlp_warmup_epochs: 5  # Training only classification head
    vision_tuning_epochs: 2  # Reduced from 5 to shorter, gentler fine-tuning of vision encoder

# Evaluation configuration
evaluation:
  metrics: ["accuracy", "balanced_accuracy", "f1_score", "precision", "recall"]
  confusion_matrix: true
  class_report: true
  visualization: true
  calibration: true
  samples_to_visualize: 20

# Output configuration
output:
  model_dir: "saved_models"
  log_dir: "logs"
  results_dir: "results"
  tensorboard: true
  checkpoint_frequency: 1
  save_best_only: true